{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nimport os\nimport re\nimport cv2\nimport pydicom\nimport nibabel as nib\nfrom sklearn.preprocessing import LabelEncoder\nfrom glob import glob\nfrom sklearn import preprocessing\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf\nfrom keras import backend as K\nimport math\ntf.config.run_functions_eagerly(True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-25T02:13:19.572312Z","iopub.execute_input":"2023-09-25T02:13:19.572669Z","iopub.status.idle":"2023-09-25T02:13:19.579578Z","shell.execute_reply.started":"2023-09-25T02:13:19.572641Z","shell.execute_reply":"2023-09-25T02:13:19.578375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    smooth = 0.0001\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_multilabel(y_true, y_pred, numLabels = 3):\n    dice = 0\n    weights = [0.348897, 16.0187006, 14.00607425]\n    for index in range(numLabels):\n        dice += dice_coef(y_true[:,:,index], y_pred[:,:,index]) * weights[index]\n    return dice/np.sum(weights)\n\ndef dice_coef_multilabelloss(y_true, y_pred):\n    return 1 - dice_coef_multilabel(y_true, y_pred)\n\ndef weightedLoss(originalLossFunc, weightsList):\n\n    def lossFunc(true, pred):\n        true = K.cast(true, K.floatx())\n        pred = K.cast(pred, K.floatx())\n\n        axis = -1 #if channels last\n          #axis=  1 #if channels first\n\n\n          #argmax returns the index of the element with the greatest value\n          #done in the class axis, it returns the class index\n        classSelectors = K.argmax(true, axis=axis)\n              #if your loss is sparse, use only true as classSelectors\n\n          #considering weights are ordered by class, for each class\n          #true(1) if the class index is equal to the weight index\n          #weightsList = tf.cast(weightsList, tf.int64)\n        classSelectors = [K.equal(tf.cast(i, tf.int64), tf.cast(classSelectors, tf.int64)) for i in range(len(weightsList))]\n\n          #casting boolean to float for calculations\n          #each tensor in the list contains 1 where ground true class is equal to its index\n          #if you sum all these, you will get a tensor full of ones.\n        classSelectors = [K.cast(x, K.floatx()) for x in classSelectors]\n\n          #for each of the selections above, multiply their respective weight\n        weights = [sel * w for sel,w in zip(classSelectors, weightsList)]\n\n          #sums all the selections\n          #result is a tensor with the respective weight for each element in predictions\n        weightMultiplier = weights[0]\n        for i in range(1, len(weights)):\n            weightMultiplier = weightMultiplier + weights[i]\n\n\n          #make sure your originalLossFunc only collapses the class axis\n          #you need the other axes intact to multiply the weights tensor\n        loss = originalLossFunc(true,pred)\n        loss = loss * weightMultiplier\n\n        return loss\n    return lossFunc","metadata":{"execution":{"iopub.status.busy":"2023-09-25T01:53:47.604767Z","iopub.execute_input":"2023-09-25T01:53:47.605060Z","iopub.status.idle":"2023-09-25T01:53:47.624985Z","shell.execute_reply.started":"2023-09-25T01:53:47.605033Z","shell.execute_reply":"2023-09-25T01:53:47.624115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Generation","metadata":{}},{"cell_type":"code","source":"metadata_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv\"\n\ntrain_metadata = pd.read_csv(metadata_path)\nsegmentations_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations\"\n\nsegmentations = os.listdir(segmentations_path)\nsegmentations = [int(os.path.splitext(segmentation)[0]) for segmentation in segmentations]\n\n\nseries = train_metadata[\"series_id\"].tolist()\n\nmatched_series = []\n\nfor segmentation in segmentations:\n    if segmentation in series:\n        matched_series.append(segmentation)\n    else:\n        continue\n\npatients_segment = train_metadata[train_metadata[\"series_id\"].isin(matched_series)].reset_index(drop=True)\npatients_with_segmentations = patients_segment[\"patient_id\"].unique()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:11:00.094338Z","iopub.execute_input":"2023-09-25T02:11:00.094711Z","iopub.status.idle":"2023-09-25T02:11:00.179754Z","shell.execute_reply.started":"2023-09-25T02:11:00.094679Z","shell.execute_reply":"2023-09-25T02:11:00.178850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_number_from_path(path):\n    match = re.search(r'(\\d+)\\.dcm$', path)\n    if match:\n        return int(match.group(1))\n    return 0\n\ndef get_data_for_3d_volumes(data, dcm_path, niftii_path):\n\n    data_to_merge = data[[\"patient_id\", \"series_id\"]]\n    \n    total_paths = []\n    patient_ids = []\n    series_ids = []\n    seg_path = []\n    \n    for patient_id in range(len(data_to_merge)):\n    \n        p_id = str(data_to_merge[\"patient_id\"][patient_id]) + \"/\" + str(data_to_merge[\"series_id\"][patient_id])\n        str_imgs_path = dcm_path + p_id + '/'\n        \n        seg_mask_paths = niftii_path + str(data_to_merge[\"series_id\"][patient_id]) + \".nii\"\n        seg_path.append(seg_mask_paths)\n        \n        patient_img_paths = []\n\n        for file in glob(str_imgs_path + '/*'):\n            patient_img_paths.append(file)\n        \n        \n        sorted_file_paths = sorted(patient_img_paths, key=extract_number_from_path)\n        total_paths.append(sorted_file_paths)\n        patient_ids.append(data_to_merge[\"patient_id\"][patient_id])\n        series_ids.append(data_to_merge[\"series_id\"][patient_id])\n    \n    final_data = pd.DataFrame(list(zip(patient_ids, series_ids, total_paths, seg_path)),\n               columns =[\"patient_id\",\"series_id\", \"patient_paths\", \"patient_segmentation\"])\n    \n    return final_data","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:11:01.975611Z","iopub.execute_input":"2023-09-25T02:11:01.976013Z","iopub.status.idle":"2023-09-25T02:11:01.986824Z","shell.execute_reply.started":"2023-09-25T02:11:01.975977Z","shell.execute_reply":"2023-09-25T02:11:01.985793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dcm_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/\"\nniftii_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations/\"\n\ncleaned_data = get_data_for_3d_volumes(patients_segment, dcm_path, niftii_path)\ncleaned_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:11:04.373751Z","iopub.execute_input":"2023-09-25T02:11:04.374314Z","iopub.status.idle":"2023-09-25T02:11:28.118827Z","shell.execute_reply.started":"2023-09-25T02:11:04.374272Z","shell.execute_reply":"2023-09-25T02:11:28.117915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def window_converter(image, window_width=400, window_level=50):\n\n    img_min = window_level - window_width // 2\n    img_max = window_level + window_width // 2\n    window_image = image.copy()\n    window_image[window_image < img_min] = img_min\n    window_image[window_image > img_max] = img_max\n    #image = (image / image.max() * 255).astype(np.float64)\n    return window_image\n\ndef transform_to_hu(medical_image, image):\n    meta_image = pydicom.dcmread(medical_image)\n    intercept = meta_image.RescaleIntercept\n    slope = meta_image.RescaleSlope\n    hu_image = image * slope + intercept\n    return hu_image\n\ndef standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n        pixel_array = dcm.pixel_array\n        if dcm.PixelRepresentation == 1:\n            bit_shift = dcm.BitsAllocated - dcm.BitsStored\n            dtype = pixel_array.dtype \n            pixel_array = (pixel_array << bit_shift).astype(dtype) >> bit_shift\n        return pixel_array\n\ndef resize_img(img_paths, target_size=(128, 128)):\n        volume_shape = (target_size[0], target_size[1], len(img_paths)) \n        volume = np.zeros(volume_shape, dtype=np.float64)\n        for i, image_path in enumerate(img_paths):\n            image = pydicom.read_file(image_path)\n            image = standardize_pixel_array(image)\n            hu_image = transform_to_hu(image_path, image)\n            window_image = window_converter(hu_image)\n            image = cv2.resize(window_image, target_size)\n            volume[:,:,i] = image\n        return volume\n    \ndef normalize_volume(resized_volume):\n    original_shape = resized_volume.shape\n    flattened_image = resized_volume.reshape((-1,))\n    scaler = preprocessing.MinMaxScaler()\n    normalized_flattened_image = scaler.fit_transform(flattened_image.reshape((-1, 1)))\n    normalized_volume_image = normalized_flattened_image.reshape(original_shape)\n    return normalized_volume_image\n\ndef create_3D_segmentations(filepath, target_size, downsample_rate=1):\n    img = nib.load(filepath).get_fdata()\n    img = np.transpose(img, [2, 1, 0])\n    img = np.rot90(img, -1, (1,2))\n    img = img[::-1,:,:]\n    img = np.transpose(img, [2, 1, 0])\n    img = img[::downsample_rate, ::downsample_rate, ::downsample_rate]\n    \n    resized_images = []\n\n    for i in range(img.shape[2]):\n        resized_img = cv2.resize(img[:, :, i], target_size)\n        resized_images.append(resized_img)\n    \n    resized_3D_mask = np.stack(resized_images, axis=2)\n    \n    return np.array(resized_3D_mask, dtype=np.int8)\n\ndef generate_patient_processed_data(list_img_paths, list_seg_paths, target_size=(128,128)):\n\n    height = target_size[0]\n    width = target_size[1]\n    depth = len(list_img_paths)\n\n    volume_array = np.zeros((height, width, depth), dtype=np.float64)\n\n    print(\"Initializing data preprocessing with the following dimensions-> Volumes:{}\".format(volume_array.shape))\n\n    resized_images = resize_img(list_img_paths, target_size=target_size)\n    normalized_siz_volume = normalize_volume(resized_images)\n    volume_array = normalized_siz_volume\n    volume_mask = create_3D_segmentations(list_seg_paths, target_size=target_size)\n\n    return volume_array, volume_mask\n\ndef compute_class_weights_and_encode_masks(volume_segmentations):\n    \n    volume_segmentations = np.transpose(volume_segmentations, (2, 0, 1))\n    # Extract unique values from labels 0, 1, and 5\n    labels_to_extract = [0, 1, 5]\n    filtered_mask = np.isin(volume_segmentations, labels_to_extract)\n\n    # Create a mask with zeros initially\n    result_mask = np.zeros_like(volume_segmentations)\n\n    # Replace the values in the result mask with the values from the filtered labels\n    for label in labels_to_extract:\n        result_mask[volume_segmentations == label] = label\n    \n    labelencoder = LabelEncoder()\n    n, h, w = result_mask.shape\n    train_masks_reshaped = result_mask.reshape(-1,1)\n    train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped.ravel())\n    train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n    number_classes = len(np.unique(train_masks_reshaped_encoded))\n\n    return train_masks_encoded_original_shape, number_classes\n\ndef transpose_and_expand_data(volume_images, volume_masks_encoded):\n    transposed_volume_dcm = np.transpose(volume_images, (2, 0, 1))\n    transposed_volume_dcm = np.expand_dims(transposed_volume_dcm, axis=3)\n    transpose_volume_nii = np.expand_dims(volume_masks_encoded, axis=3)\n    print(f\"Final data shape: {transposed_volume_dcm.shape}, {transpose_volume_nii.shape}\")\n    return transposed_volume_dcm, transpose_volume_nii\n\ndef generate_data_volumes(data, idx):\n    volume_dcm = []\n    volume_nii = []\n    for i in range(idx):\n        volume_img, volume_seg = generate_patient_processed_data(data[\"patient_paths\"][i], data[\"patient_segmentation\"][i])\n        volume_dcm.append(volume_img)\n        volume_nii.append(volume_seg)\n    volume_of_imgs = np.concatenate(volume_dcm, axis=2)\n    volume_of_segs = np.concatenate(volume_nii, axis=2)\n    return volume_of_imgs, volume_of_segs\n\ndef string_to_list(string_repr):\n    return eval(string_repr)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:14:08.752308Z","iopub.execute_input":"2023-09-25T02:14:08.752662Z","iopub.status.idle":"2023-09-25T02:14:08.780648Z","shell.execute_reply.started":"2023-09-25T02:14:08.752632Z","shell.execute_reply":"2023-09-25T02:14:08.779061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference using U-Net model","metadata":{}},{"cell_type":"code","source":"volume_of_imgs, volume_of_segs = generate_data_volumes(data=cleaned_data, idx=1)\n\nencoded_masks, num_classes = compute_class_weights_and_encode_masks(volume_of_segs)\n\nvolume_images_cleaned, volume_segs_cleaned = transpose_and_expand_data(volume_images=volume_of_imgs, volume_masks_encoded=encoded_masks)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:14:23.620461Z","iopub.execute_input":"2023-09-25T02:14:23.621587Z","iopub.status.idle":"2023-09-25T02:14:43.740716Z","shell.execute_reply.started":"2023-09-25T02:14:23.621544Z","shell.execute_reply":"2023-09-25T02:14:43.739644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(volume_images_cleaned.shape, volume_segs_cleaned.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:15:06.818103Z","iopub.execute_input":"2023-09-25T02:15:06.818992Z","iopub.status.idle":"2023-09-25T02:15:06.825203Z","shell.execute_reply.started":"2023-09-25T02:15:06.818928Z","shell.execute_reply":"2023-09-25T02:15:06.824159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks_cat = to_categorical(volume_segs_cleaned, num_classes=6)\ny_train_cat = train_masks_cat.reshape((volume_segs_cleaned.shape[0], volume_segs_cleaned.shape[1], volume_segs_cleaned.shape[2], 6))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:16:25.506312Z","iopub.execute_input":"2023-09-25T02:16:25.506669Z","iopub.status.idle":"2023-09-25T02:16:25.967181Z","shell.execute_reply.started":"2023-09-25T02:16:25.506634Z","shell.execute_reply":"2023-09-25T02:16:25.966236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading model","metadata":{}},{"cell_type":"code","source":"model = load_model('/kaggle/input/unet-model/UnetV_128_epochs.h5', custom_objects={'lossFunc': weightedLoss, 'dice_coef_multilabel': dice_coef_multilabel})","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:15:36.828049Z","iopub.execute_input":"2023-09-25T02:15:36.829051Z","iopub.status.idle":"2023-09-25T02:15:41.639665Z","shell.execute_reply.started":"2023-09-25T02:15:36.829010Z","shell.execute_reply":"2023-09-25T02:15:41.638450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = [0.17649986,  7.78453571, 41.53978194, 65.20657672, 96.75504125,  6.40743063]\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=weightedLoss(tf.keras.losses.categorical_crossentropy, class_weights), metrics=[dice_coef_multilabel, tf.keras.metrics.MeanIoU(num_classes=6), \"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:17:39.286406Z","iopub.execute_input":"2023-09-25T02:17:39.286775Z","iopub.status.idle":"2023-09-25T02:17:39.306130Z","shell.execute_reply.started":"2023-09-25T02:17:39.286744Z","shell.execute_reply":"2023-09-25T02:17:39.305211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation = model.evaluate(volume_images_cleaned, y_train_cat)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:17:42.006065Z","iopub.execute_input":"2023-09-25T02:17:42.007012Z","iopub.status.idle":"2023-09-25T02:17:48.117044Z","shell.execute_reply.started":"2023-09-25T02:17:42.006940Z","shell.execute_reply":"2023-09-25T02:17:48.116004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(volume_images_cleaned)\ny_pred_argmax=np.argmax(y_pred, axis=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:18:21.665687Z","iopub.execute_input":"2023-09-25T02:18:21.666073Z","iopub.status.idle":"2023-09-25T02:18:24.802926Z","shell.execute_reply.started":"2023-09-25T02:18:21.666040Z","shell.execute_reply":"2023-09-25T02:18:24.801888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(y_pred[300,:,:,1], cmap=\"jet\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:29:28.186386Z","iopub.execute_input":"2023-09-25T02:29:28.186756Z","iopub.status.idle":"2023-09-25T02:29:28.511800Z","shell.execute_reply.started":"2023-09-25T02:29:28.186725Z","shell.execute_reply":"2023-09-25T02:29:28.510908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 300\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\n\naxes[0].imshow(volume_images_cleaned[index,:,:], cmap='gray')\naxes[0].set_title('Slice')\n\naxes[1].imshow(volume_segs_cleaned[index,:,:], cmap='jet')\naxes[1].set_title('Ground truth')\n\naxes[2].imshow(y_pred_argmax[index,:,:], cmap='jet')\naxes[2].set_title('Prediction')\n\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:21:57.525920Z","iopub.execute_input":"2023-09-25T02:21:57.526596Z","iopub.status.idle":"2023-09-25T02:21:58.341807Z","shell.execute_reply.started":"2023-09-25T02:21:57.526563Z","shell.execute_reply":"2023-09-25T02:21:58.340892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_slices = 5\nindex = 176\n#\n# Create a figure and axes for subplots\nfig, axes = plt.subplots(num_slices, 3, figsize=(15, 4 * num_slices))\n\n# Loop through the slices and plot each one\nfor i in range(num_slices):\n    axes[i, 0].imshow(volume_images_cleaned[index + i, :, :], cmap='gray')\n    axes[i, 0].set_title(f'Slice {index + i}')\n\n    axes[i, 1].imshow(volume_segs_cleaned[index + i, :, :], cmap='jet')\n    axes[i, 1].set_title('Ground truth')\n\n    axes[i, 2].imshow(y_pred_argmax[index + i, :, :], cmap='jet')\n    axes[i, 2].set_title('Prediction')\n\n# Adjust layout and spacing\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T02:28:12.991257Z","iopub.execute_input":"2023-09-25T02:28:12.991749Z","iopub.status.idle":"2023-09-25T02:28:16.451987Z","shell.execute_reply.started":"2023-09-25T02:28:12.991710Z","shell.execute_reply":"2023-09-25T02:28:16.450861Z"},"trusted":true},"execution_count":null,"outputs":[]}]}