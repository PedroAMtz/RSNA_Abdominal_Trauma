{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# U-Net Segmentation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Data extraction","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nimport re\nimport cv2\nimport pydicom\nimport nibabel as nib\nfrom glob import glob\nfrom keras.layers import * \nfrom keras.models import Model\nfrom keras import backend as K\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\ntf.config.run_functions_eagerly(True)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:27:50.041343Z","iopub.execute_input":"2023-09-20T17:27:50.041711Z","iopub.status.idle":"2023-09-20T17:28:00.258595Z","shell.execute_reply.started":"2023-09-20T17:27:50.041683Z","shell.execute_reply":"2023-09-20T17:28:00.257320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv\"\n\ntrain_metadata = pd.read_csv(metadata_path)\ntrain_metadata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:03.241727Z","iopub.execute_input":"2023-09-20T17:28:03.242710Z","iopub.status.idle":"2023-09-20T17:28:03.278875Z","shell.execute_reply.started":"2023-09-20T17:28:03.242669Z","shell.execute_reply":"2023-09-20T17:28:03.277597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segmentations_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations\"\n\nsegmentations = os.listdir(segmentations_path)\nsegmentations = [int(os.path.splitext(segmentation)[0]) for segmentation in segmentations]\n\nprint(f\"Total number of series id that are supported with a segmentation file: {len(segmentations)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:05.610595Z","iopub.execute_input":"2023-09-20T17:28:05.611791Z","iopub.status.idle":"2023-09-20T17:28:05.650131Z","shell.execute_reply.started":"2023-09-20T17:28:05.611730Z","shell.execute_reply":"2023-09-20T17:28:05.649042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = train_metadata[\"series_id\"].tolist()\n\nmatched_series = []\n\nfor segmentation in segmentations:\n    if segmentation in series:\n        matched_series.append(segmentation)\n    else:\n        continue\n\nlen(matched_series)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:07.699052Z","iopub.execute_input":"2023-09-20T17:28:07.699771Z","iopub.status.idle":"2023-09-20T17:28:07.715977Z","shell.execute_reply.started":"2023-09-20T17:28:07.699731Z","shell.execute_reply":"2023-09-20T17:28:07.714693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patients_segment = train_metadata[train_metadata[\"series_id\"].isin(matched_series)].reset_index(drop=True)\npatients_with_segmentations = patients_segment[\"patient_id\"].unique()\npatients_segment","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:09.925831Z","iopub.execute_input":"2023-09-20T17:28:09.926262Z","iopub.status.idle":"2023-09-20T17:28:09.957142Z","shell.execute_reply.started":"2023-09-20T17:28:09.926227Z","shell.execute_reply":"2023-09-20T17:28:09.955743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we had mapped successfully the patient id and series id from the training data that we will use for segmentation purpose","metadata":{}},{"cell_type":"markdown","source":"### Data cleaning -> DICOM and NIFTII","metadata":{}},{"cell_type":"code","source":"def extract_number_from_path(path):\n    match = re.search(r'(\\d+)\\.dcm$', path)\n    if match:\n        return int(match.group(1))\n    return 0\n\ndef get_data_for_3d_volumes(data, dcm_path, niftii_path):\n\n    data_to_merge = data[[\"patient_id\", \"series_id\"]]\n    \n    total_paths = []\n    patient_ids = []\n    series_ids = []\n    seg_path = []\n    \n    for patient_id in range(len(data_to_merge)):\n    \n        p_id = str(data_to_merge[\"patient_id\"][patient_id]) + \"/\" + str(data_to_merge[\"series_id\"][patient_id])\n        str_imgs_path = dcm_path + p_id + '/'\n        \n        seg_mask_paths = niftii_path + str(data_to_merge[\"series_id\"][patient_id]) + \".nii\"\n        seg_path.append(seg_mask_paths)\n        \n        patient_img_paths = []\n\n        for file in glob(str_imgs_path + '/*'):\n            patient_img_paths.append(file)\n        \n        \n        sorted_file_paths = sorted(patient_img_paths, key=extract_number_from_path)\n        total_paths.append(sorted_file_paths)\n        patient_ids.append(data_to_merge[\"patient_id\"][patient_id])\n        series_ids.append(data_to_merge[\"series_id\"][patient_id])\n    \n    final_data = pd.DataFrame(list(zip(patient_ids, series_ids, total_paths, seg_path)),\n               columns =[\"patient_id\",\"series_id\", \"patient_paths\", \"patient_segmentation\"])\n    \n    return final_data","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:12.140770Z","iopub.execute_input":"2023-09-20T17:28:12.141166Z","iopub.status.idle":"2023-09-20T17:28:12.156376Z","shell.execute_reply.started":"2023-09-20T17:28:12.141135Z","shell.execute_reply":"2023-09-20T17:28:12.154826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dcm_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/\"\nniftii_path = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations/\"\n\ncleaned_data = get_data_for_3d_volumes(patients_segment, dcm_path, niftii_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:14.821493Z","iopub.execute_input":"2023-09-20T17:28:14.821898Z","iopub.status.idle":"2023-09-20T17:28:21.885247Z","shell.execute_reply.started":"2023-09-20T17:28:14.821870Z","shell.execute_reply":"2023-09-20T17:28:21.884358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:43.888862Z","iopub.execute_input":"2023-09-20T17:28:43.889256Z","iopub.status.idle":"2023-09-20T17:28:43.906957Z","shell.execute_reply.started":"2023-09-20T17:28:43.889226Z","shell.execute_reply":"2023-09-20T17:28:43.906147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def window_converter(image, window_width=400, window_level=50):      \n    img_min = window_level - window_width // 2\n    img_max = window_level + window_width // 2\n    window_image = image.copy()\n    window_image[window_image < img_min] = img_min\n    window_image[window_image > img_max] = img_max\n    #image = (image / image.max() * 255).astype(np.float64)\n    return window_image\n\ndef transform_to_hu(medical_image, image):\n    meta_image = pydicom.dcmread(medical_image)\n    intercept = meta_image.RescaleIntercept\n    slope = meta_image.RescaleSlope\n    hu_image = image * slope + intercept\n    return hu_image\n\ndef standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n        pixel_array = dcm.pixel_array\n        if dcm.PixelRepresentation == 1:\n            bit_shift = dcm.BitsAllocated - dcm.BitsStored\n            dtype = pixel_array.dtype \n            pixel_array = (pixel_array << bit_shift).astype(dtype) >> bit_shift\n        return pixel_array\n\ndef resize_img(img_paths, target_size=(128, 128)):\n        volume_shape = (target_size[0], target_size[1], len(img_paths)) \n        volume = np.zeros(volume_shape, dtype=np.float64)\n        for i, image_path in enumerate(img_paths):\n            image = pydicom.read_file(image_path)\n            image = standardize_pixel_array(image)\n            hu_image = transform_to_hu(image_path, image)\n            window_image = window_converter(hu_image)\n            image = cv2.resize(window_image, target_size)\n            volume[:,:,i] = image\n        return volume\n    \ndef normalize_volume(resized_volume):\n    original_shape = resized_volume.shape\n    flattened_image = resized_volume.reshape((-1,))\n    scaler = preprocessing.MinMaxScaler()\n    normalized_flattened_image = scaler.fit_transform(flattened_image.reshape((-1, 1)))\n    normalized_volume_image = normalized_flattened_image.reshape(original_shape)\n    return normalized_volume_image\n\ndef create_3D_segmentations(filepath, target_size, downsample_rate=1):\n    img = nib.load(filepath).get_fdata()\n    img = np.transpose(img, [2, 1, 0])\n    img = np.rot90(img, -1, (1,2))\n    img = img[::-1,:,:]\n    img = np.transpose(img, [2, 1, 0])\n    img = img[::downsample_rate, ::downsample_rate, ::downsample_rate]\n    \n    resized_images = []\n\n    for i in range(img.shape[2]):\n        resized_img = cv2.resize(img[:, :, i], target_size)\n        resized_images.append(resized_img)\n    \n    resized_3D_mask = np.stack(resized_images, axis=2)\n    \n    return np.array(resized_3D_mask, dtype=np.int8)\n\ndef generate_patient_processed_data(list_img_paths, list_seg_paths, target_size=(128,128)):\n\n    height = target_size[0]\n    width = target_size[1]\n    depth = len(list_img_paths)\n\n    volume_array = np.zeros((height, width, depth), dtype=np.float64)\n\n    print(\"Initializing data preprocessing with the following dimensions-> Volumes:{}\".format(volume_array.shape))\n\n    resized_images = resize_img(list_img_paths, target_size=target_size)\n    normalized_siz_volume = normalize_volume(resized_images)\n    volume_array = normalized_siz_volume\n    volume_mask = create_3D_segmentations(list_seg_paths, target_size=target_size)\n    \n    transposed_volume_dcm = np.transpose(volume_array, (2, 0, 1))\n    transpose_volume_nii = np.transpose(volume_mask, (2, 0, 1))\n    \n    labelencoder = LabelEncoder()\n\n    n, h, w = transpose_volume_nii.shape\n    train_masks_reshaped = transpose_volume_nii.reshape(-1,1)\n    train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n    train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n\n    transposed_volume_dcm = np.expand_dims(transposed_volume_dcm, axis=3)\n    transpose_volume_nii = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n\n    return transposed_volume_dcm, transpose_volume_nii","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:46.419142Z","iopub.execute_input":"2023-09-20T17:28:46.419645Z","iopub.status.idle":"2023-09-20T17:28:46.451079Z","shell.execute_reply.started":"2023-09-20T17:28:46.419605Z","shell.execute_reply":"2023-09-20T17:28:46.449815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume_dcm = []\nvolume_nii = []\n\nfor i in range(1):\n    volume_img, volume_seg = generate_patient_processed_data(cleaned_data[\"patient_paths\"][i], cleaned_data[\"patient_segmentation\"][i])\n    \n    volume_dcm.append(volume_img)\n    volume_nii.append(volume_seg)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T00:54:01.058718Z","iopub.execute_input":"2023-09-20T00:54:01.059081Z","iopub.status.idle":"2023-09-20T00:54:19.783684Z","shell.execute_reply.started":"2023-09-20T00:54:01.059051Z","shell.execute_reply":"2023-09-20T00:54:19.782672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume_of_imgs = np.concatenate(volume_dcm, axis=0)\nvolume_of_segs = np.concatenate(volume_nii, axis=0)\nvolume_of_imgs.shape, volume_of_segs.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-20T00:54:40.884673Z","iopub.execute_input":"2023-09-20T00:54:40.885047Z","iopub.status.idle":"2023-09-20T00:54:40.989080Z","shell.execute_reply.started":"2023-09-20T00:54:40.885016Z","shell.execute_reply":"2023-09-20T00:54:40.988112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segmentation = create_3D_segmentations(cleaned_data[\"patient_segmentation\"][0], target_size=(128, 128), downsample_rate=1)\n\ntranspose_volume_nii = np.transpose(segmentation, (2, 0, 1))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:28:54.293765Z","iopub.execute_input":"2023-09-20T17:28:54.294142Z","iopub.status.idle":"2023-09-20T17:29:01.860712Z","shell.execute_reply.started":"2023-09-20T17:28:54.294114Z","shell.execute_reply":"2023-09-20T17:29:01.859649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transpose_volume_nii.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:35:23.827690Z","iopub.execute_input":"2023-09-20T17:35:23.828105Z","iopub.status.idle":"2023-09-20T17:35:23.836156Z","shell.execute_reply.started":"2023-09-20T17:35:23.828072Z","shell.execute_reply":"2023-09-20T17:35:23.834667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transpose_volume_nii.dtype","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:29:04.550909Z","iopub.execute_input":"2023-09-20T17:29:04.551320Z","iopub.status.idle":"2023-09-20T17:29:04.558819Z","shell.execute_reply.started":"2023-09-20T17:29:04.551288Z","shell.execute_reply":"2023-09-20T17:29:04.557701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(transpose_volume_nii)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:29:31.314527Z","iopub.execute_input":"2023-09-20T17:29:31.315053Z","iopub.status.idle":"2023-09-20T17:29:31.764161Z","shell.execute_reply.started":"2023-09-20T17:29:31.315006Z","shell.execute_reply":"2023-09-20T17:29:31.763029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nn, h, w = transpose_volume_nii.shape\ntrain_masks_reshaped = transpose_volume_nii.reshape(-1,1)\ntrain_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\ntrain_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n\nnp.unique(train_masks_encoded_original_shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:29:53.242390Z","iopub.execute_input":"2023-09-20T17:29:53.242856Z","iopub.status.idle":"2023-09-20T17:29:54.323237Z","shell.execute_reply.started":"2023-09-20T17:29:53.242822Z","shell.execute_reply":"2023-09-20T17:29:54.322209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\ntrain_masks_input.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:29:58.473809Z","iopub.execute_input":"2023-09-20T17:29:58.474172Z","iopub.status.idle":"2023-09-20T17:29:58.481785Z","shell.execute_reply.started":"2023-09-20T17:29:58.474145Z","shell.execute_reply":"2023-09-20T17:29:58.480575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\n\nn_classes=6\n\ntrain_masks_cat = to_categorical(train_masks_input, num_classes=n_classes)\ny_train_cat = train_masks_cat.reshape((train_masks_input.shape[0], train_masks_input.shape[1], train_masks_input.shape[2], n_classes))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:30:04.052172Z","iopub.execute_input":"2023-09-20T17:30:04.052615Z","iopub.status.idle":"2023-09-20T17:30:04.573426Z","shell.execute_reply.started":"2023-09-20T17:30:04.052578Z","shell.execute_reply":"2023-09-20T17:30:04.572522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cat.dtype","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:30:07.635306Z","iopub.execute_input":"2023-09-20T17:30:07.636140Z","iopub.status.idle":"2023-09-20T17:30:07.642471Z","shell.execute_reply.started":"2023-09-20T17:30:07.636107Z","shell.execute_reply":"2023-09-20T17:30:07.641407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume = []\nfor i in range(len(y_train_cat)):\n    y = y_train_cat[i,:,:,1]\n    volume.append(y)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:39:03.802993Z","iopub.execute_input":"2023-09-20T17:39:03.804004Z","iopub.status.idle":"2023-09-20T17:39:03.811079Z","shell.execute_reply.started":"2023-09-20T17:39:03.803967Z","shell.execute_reply":"2023-09-20T17:39:03.809801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume = np.array(volume)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:39:25.171055Z","iopub.execute_input":"2023-09-20T17:39:25.171431Z","iopub.status.idle":"2023-09-20T17:39:25.225434Z","shell.execute_reply.started":"2023-09-20T17:39:25.171402Z","shell.execute_reply":"2023-09-20T17:39:25.224432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume = np.expand_dims(volume, axis=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:39:52.574552Z","iopub.execute_input":"2023-09-20T17:39:52.574964Z","iopub.status.idle":"2023-09-20T17:39:52.581282Z","shell.execute_reply.started":"2023-09-20T17:39:52.574933Z","shell.execute_reply":"2023-09-20T17:39:52.579815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume.dtype","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:41:07.774991Z","iopub.execute_input":"2023-09-20T17:41:07.775381Z","iopub.status.idle":"2023-09-20T17:41:07.783478Z","shell.execute_reply.started":"2023-09-20T17:41:07.775351Z","shell.execute_reply":"2023-09-20T17:41:07.782282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(volume)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:43:38.588186Z","iopub.execute_input":"2023-09-20T17:43:38.588612Z","iopub.status.idle":"2023-09-20T17:43:39.170684Z","shell.execute_reply.started":"2023-09-20T17:43:38.588580Z","shell.execute_reply":"2023-09-20T17:43:39.169551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(volume[300,:,:,:], cmap=\"jet\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:43:19.523364Z","iopub.execute_input":"2023-09-20T17:43:19.523804Z","iopub.status.idle":"2023-09-20T17:43:19.844032Z","shell.execute_reply.started":"2023-09-20T17:43:19.523772Z","shell.execute_reply":"2023-09-20T17:43:19.842972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(f'/kaggle/working/X_y_segmentations_data.npy', 'wb') as f:\n    np.save(f, volume_of_imgs)\n    np.save(f, volume_of_segs)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:07:32.013073Z","iopub.execute_input":"2023-09-18T18:07:32.013450Z","iopub.status.idle":"2023-09-18T18:07:34.508439Z","shell.execute_reply.started":"2023-09-18T18:07:32.013418Z","shell.execute_reply":"2023-09-18T18:07:34.507418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(f'/kaggle/working/X_y_segmentations_data.npy', 'rb') as f:\n    X = np.load(f, allow_pickle=True)\n    y = np.load(f, allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:07:36.881093Z","iopub.execute_input":"2023-09-18T18:07:36.881592Z","iopub.status.idle":"2023-09-18T18:07:37.107606Z","shell.execute_reply.started":"2023-09-18T18:07:36.881553Z","shell.execute_reply":"2023-09-18T18:07:37.106370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:07:40.608677Z","iopub.execute_input":"2023-09-18T18:07:40.609053Z","iopub.status.idle":"2023-09-18T18:07:40.616579Z","shell.execute_reply.started":"2023-09-18T18:07:40.609024Z","shell.execute_reply":"2023-09-18T18:07:40.615555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def segmentation_visualization(volume, volume_seg, slice_dcm):\n    \n    fig = plt.figure(figsize=(14,14), constrained_layout=True)\n\n    ax1 = fig.add_subplot(131)\n    ax1.imshow(volume[slice_dcm,:,:], cmap = 'gray')\n\n    ax2 = fig.add_subplot(132)\n    ax2.imshow(volume_seg[slice_dcm,:,:], cmap = 'gray')\n\n    ax3 = fig.add_subplot(133)\n    ax3.imshow(volume[slice_dcm,:,:]*np.where(volume_seg[slice_dcm,:,:]>0,1,0), cmap = 'gray')\n    ax3.set_title('Overlay of Original and Segmented', fontsize=14)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T06:34:00.753785Z","iopub.execute_input":"2023-09-17T06:34:00.754754Z","iopub.status.idle":"2023-09-17T06:34:00.767229Z","shell.execute_reply.started":"2023-09-17T06:34:00.754715Z","shell.execute_reply":"2023-09-17T06:34:00.766240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Generator","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n\n    def __init__(self, X_set: np.array, y_set: np.array, num_classes: int, batch_size: int) -> None:\n        \"\"\"_Initialization of data generator_\n\n        Parameters\n        ----------\n        X_set : np.array\n            _Set of images_\n        y_set : _np.array_\n            _Set of masks_\n        num_classes : _int_\n            _Number of classes_\n        batch_size : _int_\n            _Size of the batch taken from X_set and y_set_\n        \"\"\"\n        self.x, self.y = X_set, y_set\n        self.classes = num_classes\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n    \n    def __getitem__(self, index):\n        \"\"\"_Get item method for each epoch in training\n            and computes categorical values for\n            segmentation maks_\n\n        Parameters\n        ----------\n        index : _int_\n            _Integer utilized to return the batches_\n\n        Returns\n        -------\n        _np.array_\n            _Returns the batch from X_set and categorical btach from y_set_\n        \"\"\"\n        batch_x = self.x[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]\n\n        train_masks_cat = to_categorical(batch_y, num_classes=self.classes)\n        batch_y_categorical = train_masks_cat.reshape((batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], self.classes))\n\n        return batch_x, batch_y_categorical","metadata":{"execution":{"iopub.status.busy":"2023-09-17T05:42:57.768359Z","iopub.execute_input":"2023-09-17T05:42:57.768819Z","iopub.status.idle":"2023-09-17T05:42:58.002249Z","shell.execute_reply.started":"2023-09-17T05:42:57.768784Z","shell.execute_reply":"2023-09-17T05:42:58.001067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_gen = DataGenerator(X, y, 6, 32)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:07:31.595265Z","iopub.execute_input":"2023-09-13T17:07:31.595681Z","iopub.status.idle":"2023-09-13T17:07:31.600627Z","shell.execute_reply.started":"2023-09-13T17:07:31.595649Z","shell.execute_reply":"2023-09-13T17:07:31.599258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = data_gen[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:07:33.861784Z","iopub.execute_input":"2023-09-13T17:07:33.862181Z","iopub.status.idle":"2023-09-13T17:07:33.884097Z","shell.execute_reply.started":"2023-09-13T17:07:33.862153Z","shell.execute_reply":"2023-09-13T17:07:33.883157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:07:35.941641Z","iopub.execute_input":"2023-09-13T17:07:35.942372Z","iopub.status.idle":"2023-09-13T17:07:35.949123Z","shell.execute_reply.started":"2023-09-13T17:07:35.942338Z","shell.execute_reply":"2023-09-13T17:07:35.948110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train & Test split and categorical labels","metadata":{}},{"cell_type":"markdown","source":"### Segmentations labels\n\n\n* i. 0 = background\n* ii. 1 = liver\n* iii. 2 = spleen\n* iv. 3 = left kidney\n* v. 4 = right kidney\n* vi. 5 = bowel","metadata":{}},{"cell_type":"code","source":"segmentation_visualization(volume_of_imgs, volume_of_segs, slice_dcm=200)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T16:41:35.623535Z","iopub.execute_input":"2023-09-13T16:41:35.623994Z","iopub.status.idle":"2023-09-13T16:41:36.708673Z","shell.execute_reply.started":"2023-09-13T16:41:35.623963Z","shell.execute_reply":"2023-09-13T16:41:36.707619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_classes = len(np.unique(volume_of_segs))\n\nX_train , X_test, y_train, y_test = train_test_split(volume_of_imgs, volume_of_segs, test_size = 0.10, shuffle=True)\n\n\ntrain_masks_cat = to_categorical(y_train, num_classes=number_classes)\ny_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], number_classes))\n\n\ntest_masks_cat = to_categorical(y_test, num_classes=number_classes)\ny_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], number_classes))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:07:48.302636Z","iopub.execute_input":"2023-09-18T18:07:48.303791Z","iopub.status.idle":"2023-09-18T18:07:51.716265Z","shell.execute_reply.started":"2023-09-18T18:07:48.303750Z","shell.execute_reply":"2023-09-18T18:07:51.715252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train), len(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:08:03.698781Z","iopub.execute_input":"2023-09-18T18:08:03.699132Z","iopub.status.idle":"2023-09-18T18:08:03.705017Z","shell.execute_reply.started":"2023-09-18T18:08:03.699105Z","shell.execute_reply":"2023-09-18T18:08:03.704101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\nn, h, w, _ = volume_of_segs.shape\ntrain_masks_reshaped = volume_of_segs.reshape(-1,1)\ntrain_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped.ravel())\ntrain_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:08:48.147314Z","iopub.execute_input":"2023-09-18T18:08:48.147717Z","iopub.status.idle":"2023-09-18T18:08:50.174345Z","shell.execute_reply.started":"2023-09-18T18:08:48.147680Z","shell.execute_reply":"2023-09-18T18:08:50.173317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 classes = np.unique(train_masks_reshaped_encoded),\n                                                 y = train_masks_reshaped_encoded)\n\nprint(\"Class weights are...:\", class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:08:56.331450Z","iopub.execute_input":"2023-09-18T18:08:56.332727Z","iopub.status.idle":"2023-09-18T18:09:07.750162Z","shell.execute_reply.started":"2023-09-18T18:08:56.332675Z","shell.execute_reply":"2023-09-18T18:09:07.748563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom loss function","metadata":{}},{"cell_type":"code","source":"# Custom metrics and loss function\n\ndef dice_coef(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    smooth = 0.0001\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_multilabel(y_true, y_pred, numLabels = 6):\n    dice = 0\n    weights = [0.17649986,  7.78453571, 41.53978194, 65.20657672, 96.75504125,  6.40743063]\n    for index in range(numLabels):\n        dice += dice_coef(y_true[:,:,:,index], y_pred[:,:,:,index]) * weights[index]\n    return dice/np.sum(weights)\n\ndef dice_coef_multilabelloss(y_true, y_pred):\n    return 1 - dice_coef_multilabel(y_true, y_pred)\n\ndef weightedLoss(originalLossFunc, weightsList):\n\n    def lossFunc(true, pred):\n        true = K.cast(true, K.floatx())\n        pred = K.cast(pred, K.floatx())\n\n        axis = -1 #if channels last \n          #axis=  1 #if channels first\n\n\n          #argmax returns the index of the element with the greatest value\n          #done in the class axis, it returns the class index    \n        classSelectors = K.argmax(true, axis=axis) \n              #if your loss is sparse, use only true as classSelectors\n\n          #considering weights are ordered by class, for each class\n          #true(1) if the class index is equal to the weight index \n          #weightsList = tf.cast(weightsList, tf.int64)\n        classSelectors = [K.equal(tf.cast(i, tf.int64), tf.cast(classSelectors, tf.int64)) for i in range(len(weightsList))]\n\n          #casting boolean to float for calculations  \n          #each tensor in the list contains 1 where ground true class is equal to its index \n          #if you sum all these, you will get a tensor full of ones. \n        classSelectors = [K.cast(x, K.floatx()) for x in classSelectors]\n\n          #for each of the selections above, multiply their respective weight\n        weights = [sel * w for sel,w in zip(classSelectors, weightsList)] \n\n          #sums all the selections\n          #result is a tensor with the respective weight for each element in predictions\n        weightMultiplier = weights[0]\n        for i in range(1, len(weights)):\n            weightMultiplier = weightMultiplier + weights[i]\n\n\n          #make sure your originalLossFunc only collapses the class axis\n          #you need the other axes intact to multiply the weights tensor\n        loss = originalLossFunc(true,pred) \n        loss = loss * weightMultiplier\n\n        return loss\n    return lossFunc","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:09:15.144324Z","iopub.execute_input":"2023-09-18T18:09:15.145229Z","iopub.status.idle":"2023-09-18T18:09:15.158879Z","shell.execute_reply.started":"2023-09-18T18:09:15.145183Z","shell.execute_reply":"2023-09-18T18:09:15.157825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n        return K.mean(K.stack(prec), axis=0)\n\nLR = 0.001\n\ndef dice_coef(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef bce_dice_loss(y_true, y_pred):\n    return 0.5 * tf.keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:09:20.499048Z","iopub.execute_input":"2023-09-18T18:09:20.499928Z","iopub.status.idle":"2023-09-18T18:09:20.509474Z","shell.execute_reply.started":"2023-09-18T18:09:20.499882Z","shell.execute_reply":"2023-09-18T18:09:20.508549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## U-Net model architecture","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nnb_filter = [32,64,128,256,512]\n# Build U-Net++ model\ninputs = Input((128, 128, 1))\n#s = Lambda(lambda x: x / 255)(inputs)\nc1 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(inputs)\nc1 = Dropout(0.5)(c1)\nc1 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(c1)\nc1 = Dropout(0.5)(c1)\np1 = MaxPooling2D((2, 2), strides=(2, 2))(c1)\n\nc2 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(p1)\nc2 = Dropout(0.5)(c2)\nc2 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(c2)\nc2 = Dropout(0.5)(c2)\np2 = MaxPooling2D((2, 2), strides=(2, 2))(c2)\n\nup1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding=\"same\")(c2)\nconv1_2 = concatenate([up1_2, c1], name='merge12', axis=3)\nc3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_2)\nc3 = Dropout(0.5)(c3)\nc3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(c3)\nc3 = Dropout(0.5)(c3)\n\nconv3_1 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(p2)\nconv3_1 = Dropout(0.5)(conv3_1)\nconv3_1 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv3_1)\nconv3_1 = Dropout(0.5)(conv3_1)\npool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n\nup2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding=\"same\")(conv3_1)\nconv2_2 = concatenate([up2_2, c2], name='merge22', axis=3) #x10\nconv2_2 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_2)\nconv2_2 = Dropout(0.5)(conv2_2)\nconv2_2 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_2)\nconv2_2 = Dropout(0.5)(conv2_2)\n\nup1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding=\"same\")(conv2_2)\nconv1_3 = concatenate([up1_3, c1, c3], name='merge13', axis=3)\nconv1_3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_3)\nconv1_3 = Dropout(0.5)(conv1_3)\nconv1_3 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_3)\nconv1_3 = Dropout(0.5)(conv1_3)\n\nconv4_1 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(pool3)\nconv4_1 = Dropout(0.5)(conv4_1)\nconv4_1 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv4_1)\nconv4_1 = Dropout(0.5)(conv4_1)\npool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n\nup3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding=\"same\")(conv4_1)\nconv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=3) #x20\nconv3_2 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv3_2)\nconv3_2 = Dropout(0.5)(conv3_2)\nconv3_2 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv3_2)\nconv3_2 = Dropout(0.5)(conv3_2)\n\nup2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding=\"same\")(conv3_2)\nconv2_3 = concatenate([up2_3, c2, conv2_2], name='merge23', axis=3)\nconv2_3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_3)\nconv2_3 = Dropout(0.5)(conv2_3)\nconv2_3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_3)\nconv2_3 = Dropout(0.5)(conv2_3)\n\nup1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding=\"same\")(conv2_3)\nconv1_4 = concatenate([up1_4, c1, c3, conv1_3], name='merge14', axis=3)\nconv1_4 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_4)\nconv1_4 = Dropout(0.5) (conv1_4)\nconv1_4 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_4)\nconv1_4 = Dropout(0.5)(conv1_4)\n\nconv5_1 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(pool4)\nconv5_1 = Dropout(0.5) (conv5_1)\nconv5_1 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv5_1)\nconv5_1 = Dropout(0.5)(conv5_1)\n\nup4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding=\"same\")(conv5_1)\nconv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=3) #x30\nconv4_2 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv4_2)\nconv4_2 = Dropout(0.5)(conv4_2)\nconv4_2 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv4_2)\nconv4_2 = Dropout(0.5)(conv4_2)\n\nup3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding=\"same\")(conv4_2)\nconv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=3)\nconv3_3 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv3_3)\nconv3_3 = Dropout(0.5)(conv3_3)\nconv3_3 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv3_3)\nconv3_3 = Dropout(0.5)(conv3_3)\n\nup2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding=\"same\")(conv3_3)\nconv2_4 = concatenate([up2_4, c2, conv2_2, conv2_3], name='merge24', axis=3)\nconv2_4 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_4)\nconv2_4 = Dropout(0.5)(conv2_4)\nconv2_4 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv2_4)\nconv2_4 = Dropout(0.5)(conv2_4)\n\nup1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding=\"same\")(conv2_4)\nconv1_5 = concatenate([up1_5, c1, c3, conv1_3, conv1_4], name='merge15', axis=3)\nconv1_5 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_5)\nconv1_5 = Dropout(0.5) (conv1_5)\nconv1_5 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding=\"same\")(conv1_5)\nconv1_5 = Dropout(0.5)(conv1_5)\n\nnestnet_output_4 = Conv2D(6, (1, 1), activation='sigmoid', kernel_initializer='he_normal', padding=\"same\")(conv1_5)\nmodel = Model([inputs], [nestnet_output_4])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:09:23.064173Z","iopub.execute_input":"2023-09-18T18:09:23.064558Z","iopub.status.idle":"2023-09-18T18:09:27.523897Z","shell.execute_reply.started":"2023-09-18T18:09:23.064525Z","shell.execute_reply":"2023-09-18T18:09:27.522898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:21:59.192283Z","iopub.execute_input":"2023-09-18T05:21:59.192651Z","iopub.status.idle":"2023-09-18T05:21:59.350930Z","shell.execute_reply.started":"2023-09-18T05:21:59.192621Z","shell.execute_reply":"2023-09-18T05:21:59.350135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weigths = [0.17649986,  7.78453571, 41.53978194, 65.20657672, 96.75504125,  6.40743063]\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=dice_coef_multilabelloss, metrics=[dice_coef_multilabel, tf.keras.metrics.MeanIoU(num_classes=6)])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:09:30.641634Z","iopub.execute_input":"2023-09-18T18:09:30.641995Z","iopub.status.idle":"2023-09-18T18:09:30.669474Z","shell.execute_reply.started":"2023-09-18T18:09:30.641967Z","shell.execute_reply":"2023-09-18T18:09:30.668598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\n\nkeras.utils.plot_model(model, \"Uent++.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:20:57.687593Z","iopub.execute_input":"2023-09-18T05:20:57.687981Z","iopub.status.idle":"2023-09-18T05:20:59.028720Z","shell.execute_reply.started":"2023-09-18T05:20:57.687949Z","shell.execute_reply":"2023-09-18T05:20:59.027719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train_cat,\n                    validation_data=(X_test, y_test_cat), \n                    batch_size = 32, \n                    verbose=1, \n                    epochs=50,  \n                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T18:09:37.783444Z","iopub.execute_input":"2023-09-18T18:09:37.783855Z","iopub.status.idle":"2023-09-18T18:40:02.249938Z","shell.execute_reply.started":"2023-09-18T18:09:37.783824Z","shell.execute_reply":"2023-09-18T18:40:02.248797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_plot(metrics, history):\n    f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 4))\n    for idx, metric in enumerate(metrics):\n        ax[idx].plot(history.history[metric], ls='dashed')\n        ax[idx].set_xlabel(\"Epochs\")\n        ax[idx].set_ylabel(metric)\n        ax[idx].plot(history.history['val_' + metric]);\n        ax[idx].legend([metric, 'val_' + metric])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:40:33.826441Z","iopub.execute_input":"2023-09-18T05:40:33.826806Z","iopub.status.idle":"2023-09-18T05:40:33.833426Z","shell.execute_reply.started":"2023-09-18T05:40:33.826776Z","shell.execute_reply":"2023-09-18T05:40:33.832427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_plot(['loss', 'dice_coef_multilabel', 'mean_io_u_1'], history);","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:40:35.720489Z","iopub.execute_input":"2023-09-18T05:40:35.720845Z","iopub.status.idle":"2023-09-18T05:40:36.512240Z","shell.execute_reply.started":"2023-09-18T05:40:35.720814Z","shell.execute_reply":"2023-09-18T05:40:36.510573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of the model","metadata":{}},{"cell_type":"code","source":"Unet.save('Unet.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T06:47:10.263748Z","iopub.execute_input":"2023-09-17T06:47:10.264145Z","iopub.status.idle":"2023-09-17T06:47:10.402142Z","shell.execute_reply.started":"2023-09-17T06:47:10.264115Z","shell.execute_reply":"2023-09-17T06:47:10.401071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"volume_img_1, volume_seg_1 = generate_patient_processed_data(cleaned_data[\"patient_paths\"][12], cleaned_data[\"patient_segmentation\"][12])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T06:49:16.112658Z","iopub.execute_input":"2023-09-17T06:49:16.113460Z","iopub.status.idle":"2023-09-17T06:49:31.018304Z","shell.execute_reply.started":"2023-09-17T06:49:16.113424Z","shell.execute_reply":"2023-09-17T06:49:31.017287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transposed_volume_dcm = np.transpose(volume_img_1, (2, 0, 1))\ntransposed_volume_dcm = np.expand_dims(transposed_volume_dcm, axis=-1)\ntransposed_volume_dcm.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T06:49:33.271499Z","iopub.execute_input":"2023-09-17T06:49:33.272468Z","iopub.status.idle":"2023-09-17T06:49:33.339533Z","shell.execute_reply.started":"2023-09-17T06:49:33.272425Z","shell.execute_reply":"2023-09-17T06:49:33.338031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transpose_volume_nii = np.transpose(volume_seg, (2, 0, 1))\ntranspose_volume_nii = np.expand_dims(transpose_volume_nii, axis=3)\ntranspose_volume_nii.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-07T21:47:44.795790Z","iopub.execute_input":"2023-09-07T21:47:44.796183Z","iopub.status.idle":"2023-09-07T21:47:44.803263Z","shell.execute_reply.started":"2023-09-07T21:47:44.796142Z","shell.execute_reply":"2023-09-07T21:47:44.802137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=Unet.predict(transposed_volume_dcm)\ny_pred_argmax=np.argmax(y_pred, axis=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T21:47:46.628284Z","iopub.execute_input":"2023-09-07T21:47:46.628730Z","iopub.status.idle":"2023-09-07T21:47:48.473920Z","shell.execute_reply.started":"2023-09-07T21:47:46.628696Z","shell.execute_reply":"2023-09-07T21:47:48.472810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.metrics import MeanIoU\nn_classes = 6\nIOU_keras = MeanIoU(num_classes=n_classes)  \nIOU_keras.update_state(transpose_volume_nii, y_pred_argmax)\nprint(\"Mean IoU =\", IOU_keras.result().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-09-07T21:47:49.749578Z","iopub.execute_input":"2023-09-07T21:47:49.749948Z","iopub.status.idle":"2023-09-07T21:47:49.965790Z","shell.execute_reply.started":"2023-09-07T21:47:49.749916Z","shell.execute_reply":"2023-09-07T21:47:49.964757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OVERFITTING... next train on complete patients","metadata":{}}]}